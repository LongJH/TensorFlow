{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-143378558580>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# 载入数据集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter0, Testing Accuracy 0.1015, Training Accuracy 0.1022\n",
      "Iter100, Testing Accuracy 0.5573, Training Accuracy 0.5419\n",
      "Iter200, Testing Accuracy 0.8094, Training Accuracy 0.7914\n",
      "Iter300, Testing Accuracy 0.9247, Training Accuracy 0.9141\n",
      "Iter400, Testing Accuracy 0.9415, Training Accuracy 0.9324\n",
      "Iter500, Testing Accuracy 0.9487, Training Accuracy 0.9417\n",
      "Iter600, Testing Accuracy 0.9534, Training Accuracy 0.9528\n",
      "Iter700, Testing Accuracy 0.9594, Training Accuracy 0.9589\n",
      "Iter800, Testing Accuracy 0.9618, Training Accuracy 0.9632\n",
      "Iter900, Testing Accuracy 0.9631, Training Accuracy 0.9623\n",
      "Iter1000, Testing Accuracy 0.9669, Training Accuracy 0.967\n"
     ]
    }
   ],
   "source": [
    "# 每个批次的大小\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "# 初始化权值\n",
    "def weight_varible(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 初始化偏置\n",
    "def bias_varible(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 卷积层\n",
    "def conv2d(x, W):\n",
    "    #x input tensor of shape '[batch,in_height,in_width,in_channles]'\n",
    "    #W filter / kernel tensor of shape [filter_height,filter_width,in_channels,out_channels]\n",
    "    #`strides[0] = strides[3] = 1`. strides[1]代表x方向的步长，strides[2]代表y方向的步长\n",
    "    #padding: A `string` from: `\"SAME\", \"VALID\"`\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# 池化层\n",
    "def max_pool_2x2(x):\n",
    "    # ksize [1,x,y,1]\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    # 定义两个placeholder\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y-input')\n",
    "    with tf.name_scope('x_image'):\n",
    "        # 改变x的格式转换为4D的向量[batch, in_height, in_width, in_channels]\n",
    "        x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "with tf.name_scope('conv1'):\n",
    "    # 初始化第一个卷积层的权重和偏置\n",
    "    with tf.name_scope('W_conv1'):\n",
    "        W_conv1 = weight_varible([5, 5, 1, 32])  # 5*5采样窗口，32个卷积核从一个平面抽取特征\n",
    "    with tf.name_scope('b_conv1'):\n",
    "        b_conv1 = bias_varible([32])  # 每一个卷积核一个偏置值\n",
    "\n",
    "    # 把 x_image 和权重向量进行卷积，再加上偏置值，然后应用于 relu 激活函数\n",
    "    with tf.name_scope('conv2d_1'):\n",
    "        conv2d_1 = conv2d(x_image, W_conv1) + b_conv1\n",
    "    with tf.name_scope('relu'):\n",
    "        h_conv1 = tf.nn.relu(conv2d_1)\n",
    "    with tf.name_scope('h_pool1'):\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "with tf.name_scope('conv2'):\n",
    "    # 初始化第二个卷积层的权重和偏置\n",
    "    with tf.name_scope('W_conv2'):\n",
    "        W_conv2 = weight_varible([5, 5, 32, 64])  # 5*5采样窗口，64个卷积核从32个平面抽取特征\n",
    "    with tf.name_scope('b_conv2'):\n",
    "        b_conv2 = bias_varible([64])\n",
    "\n",
    "    # 把 h_pool1 和权重向量进行卷积，再加上偏置值，然后应用于 relu 激活函数\n",
    "    with tf.name_scope('conv2d_2'):\n",
    "        conv2d_2 = conv2d(h_pool1, W_conv2) + b_conv2\n",
    "    with tf.name_scope('h_conv2'):\n",
    "        h_conv2 = tf.nn.relu(conv2d_2)\n",
    "    \n",
    "    with tf.name_scope('h_pool2'):\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# 28*28 的图片第一次卷积后还是 28*28， 第一次池化后变为 14*14\n",
    "# 第二次卷积后为 14*14，池化后变为 7*7\n",
    "# 经过上面操作后得到 64 张 7*7 的平面\n",
    "\n",
    "with tf.name_scope('fc1'):\n",
    "    # 初始化第一个全连接层的权值\n",
    "    with tf.name_scope('W_fc1'):\n",
    "        W_fc1 = weight_varible([7*7*64, 1024])  # 生一层有 7*7*64 个神经元，全连接层有1024个神经元\n",
    "    with tf.name_scope('b_fc1'):\n",
    "        b_fc1 = bias_varible([1024])\n",
    "\n",
    "    # 把池化层2的输出扁平化为1维\n",
    "    with tf.name_scope('h_pool2_flat'):\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    # 求第一个全连接层的输出\n",
    "    with tf.name_scope('wx_plus_b_1'):\n",
    "        wx_plus_b_1 = tf.matmul(h_pool2_flat, W_fc1) + b_fc1\n",
    "    with tf.name_scope('h_fc1'):\n",
    "        h_fc1 = tf.nn.relu(wx_plus_b_1)\n",
    "\n",
    "    # keep_prob 用于表示神经元输出概率\n",
    "    with tf.name_scope('keep_prob'):\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "    with tf.name_scope('h_fc1_drop'):\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "with tf.name_scope('fc2'):\n",
    "    # 初始化第二个全连接层的权值\n",
    "    with tf.name_scope('W_fc2'):\n",
    "        W_fc2 = weight_varible([1024, 10])  # 生一层有 7*7*64 个神经元，全连接层有1024个神经元\n",
    "    with tf.name_scope('b_fc2'):\n",
    "        b_fc2 = bias_varible([10])\n",
    "    with tf.name_scope('wx_plus_b_2'):\n",
    "        wx_plus_b_2 = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    with tf.name_scope('softmax'):\n",
    "        prediction = tf.nn.softmax(wx_plus_b_2)\n",
    "\n",
    "# 交叉熵代价函数\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=prediction), name='cross_entropy')\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "    \n",
    "# 使用 AdamOptimizer 进行优化\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    # 结果存放在一个布尔型列表中\n",
    "    with tf.name_scope('correct_predition'):\n",
    "        correct_predition = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))   # argmax 返回一维张量中最大值所在的位置\n",
    "    # 求准确率\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predition, tf.float32), name='accuracy')\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# 合并所有summary\n",
    "merged = tf.summary.merge_all()\n",
    "        \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter('logs/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('logs/test', sess.graph)\n",
    "    for i in range(1001):\n",
    "        # 训练模型\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 0.5})\n",
    "        \n",
    "        # 记录训练集计算的参数\n",
    "        summary = sess.run(merged, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.0})\n",
    "        train_writer.add_summary(summary, i)\n",
    "        \n",
    "        # 记录测试集计算的参数\n",
    "        batch_xs, batch_ys = mnist.test.next_batch(batch_size)\n",
    "        summary = sess.run(merged, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.0})\n",
    "        test_writer.add_summary(summary, i)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            \n",
    "            test_acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels, keep_prob: 1.0})\n",
    "            train_acc = sess.run(accuracy, feed_dict={x:mnist.train.images[:10000], y:mnist.train.labels[:10000], keep_prob: 1.0})\n",
    "            print('Iter' + str(i) + ', Testing Accuracy ' + str(test_acc) + ', Training Accuracy ' + str(train_acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
